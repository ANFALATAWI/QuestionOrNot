{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This project attempts to predict the classification of a _sentence_ into either a question or not a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fS_BeftQ05tr"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk.data;\n",
    "from gensim.models import word2vec;\n",
    "from sklearn.cluster import KMeans;\n",
    "from sklearn.neighbors import KDTree;\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import os;\n",
    "import re;\n",
    "import logging;\n",
    "import sqlite3;\n",
    "import time;\n",
    "import sys;\n",
    "import multiprocessing;\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt;\n",
    "from itertools import cycle;\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "Here, we will be going through five steps of data analysis & building models:\n",
    "1. Collect Data\n",
    "2. Clean Data\n",
    "3. Explore & analyze Data\n",
    "4. Build models\n",
    "5. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Collect Data\n",
    "The data used in this project was collected from two different resources, one resource for data that were classify as sentences, and the other for data classify as questions.\n",
    "\n",
    "The first is [OPUS - Wikipedia](http://opus.nlpl.eu/Wikipedia.php), many datasets are avialable at OPUS, we chose the wikipedia dataset being the most general set.\n",
    "\n",
    "The other source for data that classify as questions is [this](https://github.com/vishaljain3991/cnn_topic_classification), we took all the data regardless of the domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1xNpDUHc4j3z"
   },
   "outputs": [],
   "source": [
    "# Here we need to add the steps we went through to merge two datasets together\n",
    "# All the steps, like importing wiki, cleaning & filtering, then merging it with\n",
    "# data from github.\n",
    "\n",
    "# Untill finally we reach our dataset:\n",
    "df = pd.read_csv('CPCS433Dataset.csv')\n",
    "df.rename(columns = {'sentence ':'sentence'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.Clean Data\n",
    "The steps taken in order to clean the data:\n",
    "* Change sentences to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change sentences to lower case\n",
    "df['sentence'] = df['sentence'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1461,
     "status": "ok",
     "timestamp": 1575306389709,
     "user": {
      "displayName": "نجوى ولي",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADQTtP6tio-5yIWboSl2rrfRSBvMBOCXecNkJ1=s64",
      "userId": "12542579108297309877"
     },
     "user_tz": -180
    },
    "id": "Fdgqhru34r89",
    "outputId": "d955a8cb-1e25-45e7-bbb9-48223ab6432c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>is 3hrill com brandable for a t shirt business...</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why effective app support is necessary for you...</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what business can be set up next to a car show...</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>how should i report a vending machine competitor</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how can i retain younger employees with least ...</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence type\n",
       "0  is 3hrill com brandable for a t shirt business...    Q\n",
       "1  why effective app support is necessary for you...    Q\n",
       "2  what business can be set up next to a car show...    Q\n",
       "3   how should i report a vending machine competitor    Q\n",
       "4  how can i retain younger employees with least ...    Q"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1421,
     "status": "ok",
     "timestamp": 1575306392516,
     "user": {
      "displayName": "نجوى ولي",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADQTtP6tio-5yIWboSl2rrfRSBvMBOCXecNkJ1=s64",
      "userId": "12542579108297309877"
     },
     "user_tz": -180
    },
    "id": "wa4ZS2-F4tf6",
    "outputId": "5ce09591-7994-46a2-f832-ef34bd84e6b7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16693</th>\n",
       "      <td>some republicans on the other hand have respon...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16694</th>\n",
       "      <td>jeanjacques gosso gosso born 15 march 1983 in ...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16695</th>\n",
       "      <td>aristide benoît zogbo born december 30 1981 in...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16696</th>\n",
       "      <td>siaka tiéné born 22 february 1982 is an ivoria...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16697</th>\n",
       "      <td>he is also an ivory coast international</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence type\n",
       "16693  some republicans on the other hand have respon...    S\n",
       "16694  jeanjacques gosso gosso born 15 march 1983 in ...    S\n",
       "16695  aristide benoît zogbo born december 30 1981 in...    S\n",
       "16696  siaka tiéné born 22 february 1982 is an ivoria...    S\n",
       "16697            he is also an ivory coast international    S"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1575512647616,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "0oqeO54o44Pa",
    "outputId": "55f63f1c-26e9-4079-a7f2-daf65039544c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14698</th>\n",
       "      <td>her victory in the 400 m event left silver med...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14699</th>\n",
       "      <td>she improved her record to 43610 at guayaquil ...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14700</th>\n",
       "      <td>she also held the world record in the 200 m in...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14701</th>\n",
       "      <td>she also collected a silver medal in the 400 m...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14702</th>\n",
       "      <td>she repeated her haul at the 1982 world champi...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence type\n",
       "14698  her victory in the 400 m event left silver med...    S\n",
       "14699  she improved her record to 43610 at guayaquil ...    S\n",
       "14700  she also held the world record in the 200 m in...    S\n",
       "14701  she also collected a silver medal in the 400 m...    S\n",
       "14702  she repeated her haul at the 1982 world champi...    S"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take 2000 from head\n",
    "small_df = df.iloc[:2000, :]\n",
    "# Take 2000 from tail\n",
    "small_df2 = df.iloc[-2000:,:]\n",
    "small_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1085,
     "status": "ok",
     "timestamp": 1575512651031,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "XIzbC4FM5uyz",
    "outputId": "e5fb81e1-fe99-4ed8-d6c4-d9cf236e423b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine them\n",
    "dataframe = pd.concat([small_df,small_df2])\n",
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1011,
     "status": "ok",
     "timestamp": 1575512654284,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "131JuFhs5wo7",
    "outputId": "2aaa4f0c-03d3-4781-edc0-fadec3432c92"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a few days after its release italian director ...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>why does nuvaring needs to be refrigerated</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>are there any arabic books about designing gam...</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>surprisingly he is good at making cookies and ...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he used to answer the question of whether he w...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is the book by george jordac the voice of huma...</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>its name is dedicated to bernardo de monteagud...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>why are some christians opposed to the technol...</td>\n",
       "      <td>Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>montreal swept the rangers to advance to the f...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>william ii also sent expeditions to the region...</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence type\n",
       "0  a few days after its release italian director ...    S\n",
       "1         why does nuvaring needs to be refrigerated    Q\n",
       "2  are there any arabic books about designing gam...    Q\n",
       "3  surprisingly he is good at making cookies and ...    S\n",
       "4  he used to answer the question of whether he w...    S\n",
       "5  is the book by george jordac the voice of huma...    Q\n",
       "6  its name is dedicated to bernardo de monteagud...    S\n",
       "7  why are some christians opposed to the technol...    Q\n",
       "8  montreal swept the rangers to advance to the f...    S\n",
       "9  william ii also sent expeditions to the region...    S"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle data\n",
    "dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10073,
     "status": "ok",
     "timestamp": 1575512669666,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "G0PwkAIm_cjN",
    "outputId": "247cfe24-a55f-4a03-a6e7-6cca462f4916"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/anfal/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-91a0d7956fae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfreqDist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \"\"\"\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     return [\n\u001b[1;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nltk'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'file'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.7/lib/python/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/anfal/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/share/nltk_data'\n    - '/Library/Frameworks/Python.framework/Versions/3.7/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# import nltk.data;\n",
    "# nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# The frequency distribution of the words\n",
    "sentences = dataframe.sentence.str.cat(sep=' ')\n",
    "\n",
    "tokens = nltk.word_tokenize(sentences)\n",
    "\n",
    "freqDist = nltk.FreqDist(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4508,
     "status": "ok",
     "timestamp": 1575512678948,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "SPnWEHR6Xubt",
    "outputId": "10aee575-78d3-4e6d-e934-2e2d86d680af"
   },
   "outputs": [],
   "source": [
    "df['tokenized_sents'] = df.apply(lambda row: nltk.word_tokenize(row['sentence']), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZoiUqQotPdC"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-9f1899db5e3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68998,
     "status": "ok",
     "timestamp": 1575294489860,
     "user": {
      "displayName": "نجوى ولي",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADQTtP6tio-5yIWboSl2rrfRSBvMBOCXecNkJ1=s64",
      "userId": "12542579108297309877"
     },
     "user_tz": -180
    },
    "id": "aSMwLAVctsX1",
    "outputId": "2f495f05-f1fc-48a0-8ac5-25dc7159359c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "# download the model and return as object ready for use\n",
    "model_glove_twitter = api.load(\"glove-twitter-25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 71191,
     "status": "ok",
     "timestamp": 1575512756911,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "z86hRzzZt_JZ",
    "outputId": "7d8e94d7-5d7a-43f5-cb8a-7e7a403e687a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1245,
     "status": "ok",
     "timestamp": 1575294729162,
     "user": {
      "displayName": "نجوى ولي",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADQTtP6tio-5yIWboSl2rrfRSBvMBOCXecNkJ1=s64",
      "userId": "12542579108297309877"
     },
     "user_tz": -180
    },
    "id": "pUkjAoLhvM6y",
    "outputId": "e068c5ab-afde-42f6-e11e-1bff91d0bc59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('plane', 0.8651753067970276),\n",
       " ('jet', 0.8155897259712219),\n",
       " ('aircraft', 0.7861484289169312),\n",
       " ('airplanes', 0.7662244439125061),\n",
       " ('airliner', 0.7646356821060181),\n",
       " ('jetliner', 0.7469576597213745),\n",
       " ('flight', 0.733950138092041),\n",
       " ('planes', 0.733876645565033),\n",
       " ('flying', 0.7300053834915161),\n",
       " ('crash', 0.7099080085754395)]"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gigaword.wv.most_similar(\"airplane\",topn=10)\n",
    "# Other pretrained:\n",
    "# word2vec-google-news-300\n",
    "# word2vec-ruscorpora-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAQAZONivofj"
   },
   "outputs": [],
   "source": [
    "model_gigaword.wv['hello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5YYHeKPAxGxX"
   },
   "outputs": [],
   "source": [
    "sentence = ['london', \"is\", \"the\", \"capital\", \"of\", \"great\", \"britain\"]\n",
    "vectors = [model_gigaword[w] for w in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32289,
     "status": "ok",
     "timestamp": 1575512800783,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "qhlrLMnm7EWG",
    "outputId": "1f220bc3-79ca-46d4-aea8-131ce3a34707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_md==2.1.0\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz (95.4MB)\n",
      "\u001b[K     |████████████████████████████████| 95.4MB 1.2MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: en-core-web-md\n",
      "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for en-core-web-md: filename=en_core_web_md-2.1.0-cp36-none-any.whl size=97126237 sha256=fc8a37d8725ce51dfe0a6b9c82fcbbd763ac74e0aa80df8da726f824a548810b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1y1g6ycg/wheels/c1/2c/5f/fd7f3ec336bf97b0809c86264d2831c5dfb00fc2e239d1bb01\n",
      "Successfully built en-core-web-md\n",
      "Installing collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-2.1.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMP3bNgR-WqI"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 921,
     "status": "ok",
     "timestamp": 1575512808195,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "Vd2aDx1W64GX",
    "outputId": "d1664482-8c31-4bf4-e947-54120cf1d4db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.57058   0.44183   0.70102  -0.41713  -0.34058   0.02339  -0.071537\n",
      "  0.48177  -0.013121  0.16834  -0.13389   0.040626  0.15827  -0.44342\n",
      " -0.019403 -0.009661 -0.046284  0.093228 -0.27331   0.2285    0.33089\n",
      " -0.36474   0.078741  0.3585    0.44757  -0.2299    0.18077  -0.6265\n",
      "  0.053852 -0.29154  -0.4256    0.62903   0.14393  -0.046004 -0.21007\n",
      "  0.48879  -0.057698  0.37431  -0.030075 -0.34494  -0.29702   0.15095\n",
      "  0.28248  -0.16578   0.076131 -0.093016  0.79365  -0.60489  -0.18874\n",
      " -1.0173    0.31962  -0.16344   0.54177   1.1725   -0.47875  -3.3842\n",
      " -0.081301 -0.3528    1.8372    0.44516  -0.52666   0.99786  -0.32178\n",
      "  0.033462  1.1783   -0.072905  0.39737   0.26166   0.33111  -0.35629\n",
      " -0.16558  -0.44382  -0.14183  -0.37976   0.28994  -0.029114 -0.35169\n",
      " -0.27694  -1.344     0.19555   0.16887   0.040237 -0.80212   0.23366\n",
      " -1.3837   -0.023132  0.085395 -0.74051  -0.073934 -0.58838  -0.085735\n",
      " -0.10525  -0.51571   0.15038  -0.16694  -0.16372  -0.22702  -0.66102\n",
      "  0.47197   0.37253 ]\n",
      "Next:\n",
      "[-0.54264    0.41476    1.0322    -0.40244    0.46691    0.21816\n",
      " -0.074864   0.47332    0.080996  -0.22079   -0.12808   -0.1144\n",
      "  0.50891    0.11568    0.028211  -0.3628     0.43823    0.047511\n",
      "  0.20282    0.49857   -0.10068    0.13269    0.16972    0.11653\n",
      "  0.31355    0.25713    0.092783  -0.56826   -0.52975   -0.051456\n",
      " -0.67326    0.92533    0.2693     0.22734    0.66365    0.26221\n",
      "  0.19719    0.2609     0.18774   -0.3454    -0.42635    0.13975\n",
      "  0.56338   -0.56907    0.12398   -0.12894    0.72484   -0.26105\n",
      " -0.26314   -0.43605    0.078908  -0.84146    0.51595    1.3997\n",
      " -0.7646    -3.1453    -0.29202   -0.31247    1.5129     0.52435\n",
      "  0.21456    0.42452   -0.088411  -0.17805    1.1876     0.10579\n",
      "  0.76571    0.21914    0.35824   -0.11636    0.093261  -0.62483\n",
      " -0.21898    0.21796    0.74056   -0.43735    0.14343    0.14719\n",
      " -1.1605    -0.050508   0.12677   -0.014395  -0.98676   -0.091297\n",
      " -1.2054    -0.11974    0.047847  -0.54001    0.52457   -0.70963\n",
      " -0.32528   -0.1346    -0.41314    0.33435   -0.0072412  0.32253\n",
      " -0.044219  -1.2969     0.76217    0.46349  ]\n",
      "Next:\n",
      "[-1.4452e-01  5.6023e-01  2.0540e-01 -2.6664e-01 -2.5669e-01  5.2548e-01\n",
      " -4.5982e-01  2.1998e-01 -1.3937e-02 -2.4225e-01  1.8083e-01  7.1322e-02\n",
      "  3.8370e-01 -3.6641e-01  5.1304e-01 -3.7246e-01 -1.9897e-01 -5.5156e-02\n",
      "  2.0737e-02  7.0062e-01  8.1707e-01 -9.9955e-02  1.6988e-01 -2.5001e-01\n",
      " -2.6900e-01 -6.9777e-01 -3.5842e-02 -5.0822e-01 -5.9415e-03 -3.2951e-01\n",
      "  1.8912e-01  8.4527e-02 -4.3161e-01 -2.8211e-02 -1.9869e-01  4.7913e-01\n",
      "  6.4726e-02  2.9788e-01 -2.7285e-01  1.2602e-01 -9.6007e-01 -4.5345e-01\n",
      "  1.8871e-01 -4.1984e-01  4.6052e-02 -2.4216e-01  4.7940e-01 -2.2272e-01\n",
      " -5.5945e-02 -4.7897e-01  9.0283e-02 -5.9132e-02 -2.4845e-01  1.0303e+00\n",
      " -4.3041e-01 -2.4090e+00  1.7499e-01 -6.1541e-01  1.8243e+00  7.7120e-01\n",
      "  2.4063e-01  1.6522e+00 -3.9428e-02  5.1181e-01  7.2804e-01 -3.0182e-01\n",
      "  8.1372e-01  3.2864e-01  6.7350e-01 -3.9717e-01 -4.5192e-01 -3.8743e-02\n",
      " -1.1955e-01 -2.1259e-01  2.0072e-01  2.9862e-01 -3.0866e-02  1.8320e-01\n",
      " -1.3020e+00  1.9520e-01  1.1924e+00 -3.8824e-01 -3.3065e-01  1.4674e-03\n",
      " -1.7257e+00  1.0662e-01  3.1828e-01  1.0223e-02 -4.3216e-01 -1.5430e-01\n",
      "  2.2103e-01 -1.4726e-01 -4.9401e-01 -4.1887e-01 -1.0336e+00 -3.4403e-01\n",
      " -7.7463e-01 -3.6395e-01  9.1609e-01  8.2095e-01]\n",
      "Next:\n",
      "[-0.49705    0.71642    0.40119   -0.05761    0.83614    0.8256\n",
      "  0.08963   -0.53492    0.34335   -0.27079   -0.011152   0.025207\n",
      " -0.1235     0.11801    0.045312   0.73144    0.13744   -0.13084\n",
      " -0.028249  -0.30789   -0.81864   -0.54517    0.25151    0.53891\n",
      "  0.38293   -1.0343    -0.1104     0.44977   -0.13019    0.24847\n",
      "  0.1048     0.19567   -0.42672   -0.37912    0.14535   -0.025532\n",
      " -0.23523   -0.3638    -0.14269    0.0062072 -0.63      -0.23068\n",
      "  0.086461   0.22126   -0.65625   -0.55701   -0.60243   -0.13159\n",
      " -0.027226   0.0044152  1.4123     1.3042     0.54118    0.33443\n",
      " -0.51865   -1.8253    -0.30525   -0.32747    1.236      0.08771\n",
      "  0.007793   0.36571   -0.39304   -0.79174    0.57874   -0.0025427\n",
      "  0.10442    0.64166   -0.1881    -0.76203    0.23008    0.30637\n",
      "  1.0386    -0.69846    0.31094    0.63762   -0.09997    0.16999\n",
      " -0.59984   -0.89565   -0.25059   -0.93011   -0.59606   -0.32965\n",
      " -1.6828     0.39102    0.65383   -1.5176     0.61748    0.0075596\n",
      "  0.040066   0.60803   -0.027058   0.15273   -0.16887   -0.47664\n",
      " -0.61775   -0.98735    0.23776    0.39952  ]\n",
      "Next:\n",
      "(400,)\n"
     ]
    }
   ],
   "source": [
    "# Access vectors for specific words with a keyed lookup:\n",
    "vector = model_gigaword['easy']\n",
    "# see the shape of the vector (300,)\n",
    "vector.shape\n",
    "# Processing sentences is not as simple as with Spacy:\n",
    "vectors = [model_gigaword[x] for x in [\"this\", \"is\", \"some\", \"text\"]]\n",
    "vec = []\n",
    "for v in vectors:\n",
    "  vec = np.concatenate((vec, v), axis=0)\n",
    "  print(v)\n",
    "  print(\"Next:\")\n",
    "\n",
    "print(vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 748,
     "status": "ok",
     "timestamp": 1575489670568,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "boQYnbAx7qGu",
    "outputId": "4d0a2971-472e-4962-8b2f-4fb51420ed4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gigaword.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oqXLMPZ5OJE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZ8NiVPK706U"
   },
   "outputs": [],
   "source": [
    "def sentence_into_vec(df):\n",
    "  \n",
    "  # Iterate over each row\n",
    "  for index, row in df.iterrows():\n",
    "    # Extract cell\n",
    "    sent = row['tokenized_sents']\n",
    "\n",
    "    # Vectorize\n",
    "    try:\n",
    "      vectors = [model_gigaword[x] for x in sent]\n",
    "    except:\n",
    "      print(\"Word not found in vocab.\")\n",
    "\n",
    "    vec = []\n",
    "    for v in vectors:\n",
    "      vec = np.concatenate((vec, v), axis=0)\n",
    "    print(vec.shape)\n",
    "\n",
    "    # Save vector over cell\n",
    "    row['tokenized_sents'] = vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1516,
     "status": "ok",
     "timestamp": 1575489709282,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "O4RLUgp2CBkC",
    "outputId": "c22d3e42-df2a-4edf-8902-e580a75cfe0e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>type</th>\n",
       "      <th>tokenized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>with technology innovation and new business mo...</td>\n",
       "      <td>Q</td>\n",
       "      <td>[-0.4360800087451935, 0.39103999733924866, 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the landscapes of earth during the quaternary ...</td>\n",
       "      <td>S</td>\n",
       "      <td>[-0.03819400072097778, -0.24487000703811646, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lee has also appeared as harry tang chucks off...</td>\n",
       "      <td>S</td>\n",
       "      <td>[-0.03819400072097778, -0.24487000703811646, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which are the best books for amie section a exam</td>\n",
       "      <td>Q</td>\n",
       "      <td>[0.030239999294281006, 0.4460600018501282, 0.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what is the best way to prepare a chicken mars...</td>\n",
       "      <td>Q</td>\n",
       "      <td>[-0.1518000066280365, 0.38409000635147095, 0.8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  ...                                    tokenized_sents\n",
       "0  with technology innovation and new business mo...  ...  [-0.4360800087451935, 0.39103999733924866, 0.5...\n",
       "1  the landscapes of earth during the quaternary ...  ...  [-0.03819400072097778, -0.24487000703811646, 0...\n",
       "2  lee has also appeared as harry tang chucks off...  ...  [-0.03819400072097778, -0.24487000703811646, 0...\n",
       "3   which are the best books for amie section a exam  ...  [0.030239999294281006, 0.4460600018501282, 0.4...\n",
       "4  what is the best way to prepare a chicken mars...  ...  [-0.1518000066280365, 0.38409000635147095, 0.8...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1209,
     "status": "ok",
     "timestamp": 1575313619152,
     "user": {
      "displayName": "نجوى ولي",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mADQTtP6tio-5yIWboSl2rrfRSBvMBOCXecNkJ1=s64",
      "userId": "12542579108297309877"
     },
     "user_tz": -180
    },
    "id": "TY1a95o8FA0H",
    "outputId": "16a8f5ec-eca5-4fb6-dc2c-6db7480058db"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>type</th>\n",
       "      <th>tokenized_sents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>why do i get fat even though i don t eat much</td>\n",
       "      <td>Q</td>\n",
       "      <td>[0.18449999392032623, 0.5146099925041199, 0.65...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the montreal canadiens won their first stanley...</td>\n",
       "      <td>S</td>\n",
       "      <td>[-0.03819400072097778, -0.24487000703811646, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bill durnan was the last goaltender in nhl his...</td>\n",
       "      <td>S</td>\n",
       "      <td>[-0.10535000264644623, -0.02504800073802471, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paolo bianco born 20 august 1977 is an italian...</td>\n",
       "      <td>S</td>\n",
       "      <td>[0.9362000226974487, -0.44703999161720276, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who has the best dropshipping service for hair...</td>\n",
       "      <td>Q</td>\n",
       "      <td>[0.9362000226974487, -0.44703999161720276, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16693</th>\n",
       "      <td>what are the different possible strategies of ...</td>\n",
       "      <td>Q</td>\n",
       "      <td>[-0.1518000066280365, 0.38409000635147095, 0.8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16694</th>\n",
       "      <td>how do i book a flat in rome for free for visa...</td>\n",
       "      <td>Q</td>\n",
       "      <td>[-0.2376900017261505, 0.5939199924468994, 0.58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16695</th>\n",
       "      <td>i have recently published a book and want to g...</td>\n",
       "      <td>Q</td>\n",
       "      <td>[-0.046539001166820526, 0.6196600198745728, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16696</th>\n",
       "      <td>coös county  with two syllables frequently spe...</td>\n",
       "      <td>S</td>\n",
       "      <td>[-0.046539001166820526, 0.6196600198745728, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16697</th>\n",
       "      <td>what experts are saying about thermoform packa...</td>\n",
       "      <td>Q</td>\n",
       "      <td>[-0.046539001166820526, 0.6196600198745728, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16698 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  ...                                    tokenized_sents\n",
       "0          why do i get fat even though i don t eat much  ...  [0.18449999392032623, 0.5146099925041199, 0.65...\n",
       "1      the montreal canadiens won their first stanley...  ...  [-0.03819400072097778, -0.24487000703811646, 0...\n",
       "2      bill durnan was the last goaltender in nhl his...  ...  [-0.10535000264644623, -0.02504800073802471, 0...\n",
       "3      paolo bianco born 20 august 1977 is an italian...  ...  [0.9362000226974487, -0.44703999161720276, -0....\n",
       "4      who has the best dropshipping service for hair...  ...  [0.9362000226974487, -0.44703999161720276, -0....\n",
       "...                                                  ...  ...                                                ...\n",
       "16693  what are the different possible strategies of ...  ...  [-0.1518000066280365, 0.38409000635147095, 0.8...\n",
       "16694  how do i book a flat in rome for free for visa...  ...  [-0.2376900017261505, 0.5939199924468994, 0.58...\n",
       "16695  i have recently published a book and want to g...  ...  [-0.046539001166820526, 0.6196600198745728, 0....\n",
       "16696  coös county  with two syllables frequently spe...  ...  [-0.046539001166820526, 0.6196600198745728, 0....\n",
       "16697  what experts are saying about thermoform packa...  ...  [-0.046539001166820526, 0.6196600198745728, 0....\n",
       "\n",
       "[16698 rows x 3 columns]"
      ]
     },
     "execution_count": 168,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tokenized_sents'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 732
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1505,
     "status": "error",
     "timestamp": 1575489693619,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "wwH59i12In1x",
    "outputId": "7586aa69-fe7a-4b43-b6b1-c73a56d812b5"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4735\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4736\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlibindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value_box\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4737\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_box\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.get_value_at\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/util.pxd\u001b[0m in \u001b[0;36mpandas._libs.util.validate_indexer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-c2bd273467e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmax_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenized_sents'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mcurrent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4742\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4743\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4744\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4745\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4746\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'tokenized_sents'"
     ]
    }
   ],
   "source": [
    "max_shape = 0\n",
    "for index, row in dataframe.iterrows():\n",
    "    sent = row['tokenized_sents']\n",
    "    current = sent.shape[0]\n",
    "    if current > max_shape:\n",
    "      max_shape = current\n",
    "print(max_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 97
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2484,
     "status": "ok",
     "timestamp": 1575512833091,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "CJ0UvYK5JAvs",
    "outputId": "d88606b5-d8d8-4819-8486-b315389ebb3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style=\"color: red;\">\n",
       "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
       "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
       "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
       "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "16698"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "# First Encode label \n",
    "encoder = preprocessing.LabelEncoder()\n",
    "df['type'] = encoder.fit_transform(df['type'])\n",
    "\n",
    "features = df['tokenized_sents']\n",
    "label = df['type']\n",
    "\n",
    "featuresP = pad_sequences(features, padding='post', dtype='float32')\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(featuresP, label, train_size=0.8)\n",
    "\n",
    "len(featuresP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ibki1L-TzlV8"
   },
   "outputs": [],
   "source": [
    "#Train SVM Model\n",
    "from sklearn import model_selection, svm\n",
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(x_train,y_train)\n",
    "predictions_SVM = SVM.predict(x_test)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 818,
     "status": "ok",
     "timestamp": 1575514333157,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "pmlsdbLH7Did",
    "outputId": "8ca825f3-483b-4aab-9b0d-b82a02fd1d92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8263473053892215\n",
      "Error Rate:  0.17365269461077848\n",
      "Precision:  0.8730048577376822\n",
      "Recall:  0.7601208459214501\n",
      "Time:  1447.0216181690012\n",
      "F1 =  0.8254155707825765\n",
      "Confusion matrix [[1502  183]\n",
      " [ 397 1258]]\n"
     ]
    }
   ],
   "source": [
    "# Get accuracy\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "acc = accuracy_score(predictions_SVM, y_test)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Error Rate: \", 1 - acc)\n",
    "print(\"Precision: \", precision_score(y_test, predictions_SVM))\n",
    "print(\"Recall: \", recall_score(y_test, predictions_SVM))\n",
    "print(\"Time: \",stop - start)\n",
    "print(\"F1 = \", f1_score(y_test, predictions_SVM , average=\"macro\"))\n",
    "print(\"Confusion matrix\", confusion_matrix(y_test, predictions_SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 494348,
     "status": "ok",
     "timestamp": 1575492695117,
     "user": {
      "displayName": "Bayader",
      "photoUrl": "",
      "userId": "00300479938631670664"
     },
     "user_tz": -180
    },
    "id": "9_5oALQJKxLq",
    "outputId": "765065ce-048a-4913-fb95-998fc5765d2d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8952687528073064"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train, SVM.predict(x_train))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SVM_QuestionOrNot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
